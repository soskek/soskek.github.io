<!DOCTYPE html>
<!--
        Strata by HTML5 UP
        html5up.net | @ajlkn
        Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<head>
  <meta name="generator" content=
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src=
  "https://www.googletagmanager.com/gtag/js?id=UA-62879081-3"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-62879081-3');
  </script>
  <title>Sosuke Kobayashi</title>
  <meta name="viewport" content=
        "width=device-width, initial-scale=1, user-scalable=no">
  <link rel="stylesheet" href="assets/css/academicons.min.css">
  <link rel="stylesheet" href="assets/css/main.css">
</head>
<body class="is-preload">
  <!-- Header -->
  <header id="header">
    <div class="inner">
      <!-- <a href="#" class="image avatar"><img src="./img/face.png"
      alt=""></a> -->
    <!--</div>
        <div class="inner">-->
          <br>
          <ul class="icons">
        <li>
          <a class="icon brands social-icon" href="https://scholar.google.com/citations?user=VY6PqvsAAAAJ" target="_blank" rel="noopener" title="Google Scholar">
            <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
              <path d="M15.7399177,5.48151786 C15.149924,4.82272859 14.4663239,4.49480724 13.6933878,4.49480724 C12.8185607,4.49480724 12.1471312,4.80966127 11.6791845,5.43796013 C11.2112379,6.06433732 10.9771365,6.82326666 10.9771365,7.71483354 C10.9771365,8.47423262 11.1052474,9.24733956 11.3616402,10.035649 C11.6170934,10.8236168 12.0350768,11.5270314 12.6210136,12.1470458 C13.2049434,12.7685974 13.8844439,13.0787968 14.6564833,13.0787968 C15.5171327,13.0787968 16.189459,12.7903763 16.671626,12.2137916 C17.1517432,11.6379755 17.3927199,10.9111166 17.3927199,10.035649 C17.3927199,9.29000051 17.2655484,8.50924961 17.0102233,7.69296927 C16.7561365,6.87562134 16.3319184,6.13812924 15.739875,5.48156056 L15.7399177,5.48151786 Z M24.3072528,4.96950105 L24.3072528,13.3462071 C24.3072528,13.7339989 23.9898792,14.0515006 23.6019592,14.0515006 L23.3422783,14.0515006 C22.9543156,14.0515006 22.6369848,13.7341271 22.6369848,13.3462071 L22.6369848,4.96950105 C22.6369848,4.62513879 22.5869788,4.33765779 23.068249,4.27714671 L23.068249,3.16262405 L19.3634081,6.20120253 C19.4061972,6.28058863 19.446851,6.33132057 19.4854124,6.39789556 C19.8110278,6.97418137 19.9768461,7.69087679 19.9768461,8.56681414 C19.9768461,9.23824368 19.8649625,9.84134739 19.638078,10.3740328 C19.4122611,10.9065473 19.1375912,11.3415267 18.817143,11.6765369 C18.496652,12.0127855 18.1752643,12.3201237 17.8538766,12.5967152 C17.5324462,12.8739047 17.2577763,13.162923 17.0328135,13.4629589 C16.8060571,13.7620125 16.6932767,14.0712297 16.6932767,14.3917206 C16.6932767,14.7126813 16.8397076,15.0387237 17.1317151,15.3673283 C17.4226551,15.6968724 17.7805972,16.0162103 18.2038758,16.3317049 C18.6279231,16.6450216 19.0510736,16.9928002 19.4743522,17.3712399 C19.8985703,17.74921 20.2545052,18.2354765 20.5454452,18.8259399 C20.8385631,19.4186239 20.9848231,20.071093 20.9848231,20.7882582 C20.9848231,21.7343577 20.7435475,22.5887297 20.2626616,23.3491963 C19.7802812,24.1064175 19.1518542,24.7106315 18.3796867,25.1537246 C17.6054695,25.599807 16.777531,25.9355858 15.8945476,26.1623422 C15.0097279,26.3870488 14.1317408,26.5 13.2548639,26.5 C12.7014246,26.5 12.1429889,26.4573818 11.5814359,26.3697112 C11.0178757,26.2822114 10.4532907,26.1276668 9.8847343,25.9098782 C9.31502485,25.6906377 8.81052389,25.4214338 8.37319577,25.0981672 C7.93475736,24.778146 7.58181166,24.3652016 7.3111559,23.862238 C7.04054285,23.3591036 6.90615445,22.7936645 6.90615445,22.1655364 C6.90615445,21.4203577 7.1136515,20.7291563 7.52975591,20.0847582 C7.94586031,19.4451002 8.49707907,18.9108775 9.18272893,18.487727 C10.3789864,17.7434877 12.2558547,17.2836547 14.8102164,17.1098936 C14.2262867,16.3799173 13.9333396,15.6928582 13.9333396,15.0498693 C13.9333396,14.6841125 14.0289531,14.2920503 14.218173,13.8687717 C13.9130554,13.9113899 13.5987992,13.9355175 13.2781801,13.9355175 C11.9059836,13.9355175 10.7472627,13.4894351 9.80628769,12.5911638 C8.86535538,11.6944724 8.39535897,10.5719642 8.39535897,9.21488479 C8.39535897,9.07298054 8.39941581,8.94802965 8.40970739,8.8095417 L2.83316537,8.8095417 L11.2417283,1.5 L25.1668346,1.5 L23.87996,2.50451805 L23.87996,4.27748834 C24.3577712,4.3388962 24.3072955,4.62607827 24.3072955,4.96950105 L24.3072528,4.96950105 Z M15.8172967,17.9839947 C15.6566455,17.9549562 15.430914,17.9386434 15.1399313,17.9386434 C14.512273,17.9386434 13.8947782,17.9941154 13.2885571,18.1048887 C12.6822934,18.2126727 12.084186,18.3947184 11.4941496,18.6515809 C10.9019353,18.906906 10.423996,19.2823566 10.0597766,19.7787865 C9.69355004,20.274576 9.51150436,20.858463 9.51150436,21.5303623 C9.51150436,22.1706609 9.67215551,22.7419077 9.99371402,23.2394479 C10.3142049,23.7338281 10.7373127,24.1208513 11.2632509,24.3980408 C11.7891891,24.6756999 12.3405787,24.8854176 12.916224,25.0231795 C13.4929795,25.1600875 14.0799412,25.2311463 14.6780913,25.2311463 C15.8610679,25.2311463 16.8794647,24.9647609 17.7338794,24.4320756 C18.5863724,23.899561 19.0135798,23.0772595 19.0135798,21.9672634 C19.0135798,21.7338025 18.9810823,21.5040141 18.916856,21.2796064 C18.8498112,21.0527219 18.7835352,20.8585057 18.7185829,20.6972567 C18.6534171,20.5389543 18.529363,20.3483679 18.3462498,20.1286576 C18.1642041,19.9083067 18.0248194,19.7455631 17.9310848,19.6372239 C17.8354714,19.5253403 17.6565431,19.3656714 17.3929761,19.1545445 C17.131587,18.9425209 16.9647012,18.8103104 16.8904822,18.7614574 C16.8172027,18.7100422 16.6270434,18.5711699 16.3207728,18.3458227 C16.0147157,18.1186393 15.8468903,17.9981723 15.8173394,17.983952 L15.8172967,17.9839947 Z"></path>
            </svg>
          </a>
        </li>
        <li>
          <a class="icon brands social-icon" href="https://github.com/soskek" target="_blank" rel="noopener" title="GitHub">
            <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
              <path d="M13.9988029,1.32087331 C6.82105037,1.32087331 1,7.14112562 1,14.3212723 C1,20.0649109 4.72454649,24.9370678 9.89038951,26.6560892 C10.5408085,26.7757983 10.7778323,26.374374 10.7778323,26.0296121 C10.7778323,25.7215609 10.7666595,24.9035493 10.760275,23.8189856 C7.14426471,24.6042767 6.38131925,22.0760223 6.38131925,22.0760223 C5.78995672,20.5740732 4.93762853,20.1742451 4.93762853,20.1742451 C3.75729765,19.3682044 5.02701126,19.3841656 5.02701126,19.3841656 C6.33183953,19.4759425 7.01817121,20.7241085 7.01817121,20.7241085 C8.17775254,22.7104801 10.0611744,22.1366749 10.8017741,21.8038838 C10.919887,20.9643246 11.2558703,20.3913175 11.6269683,20.066507 C8.74038491,19.7385043 5.70536235,18.6228163 5.70536235,13.6413251 C5.70536235,12.2223743 6.21213051,11.0611968 7.04370914,10.1530044 C6.90963504,9.82420367 6.46351945,8.50181809 7.17139875,6.71256734 C7.17139875,6.71256734 8.26234691,6.36301702 10.7459099,8.04532771 C11.78259,7.75642995 12.8950858,7.61277914 14.000399,7.60719272 C15.1049142,7.61277914 16.2166119,7.75642995 17.2548881,8.04532771 C19.736855,6.36301702 20.8262071,6.71256734 20.8262071,6.71256734 C21.5356825,8.50181809 21.0895669,9.82420367 20.9562909,10.1530044 C21.7894656,11.0611968 22.2922435,12.2223743 22.2922435,13.6413251 C22.2922435,18.6355852 19.2524325,19.734514 16.3570705,20.0561322 C16.8231376,20.4575564 17.2389269,21.2508282 17.2389269,22.4638795 C17.2389269,24.2012564 17.2229657,25.603448 17.2229657,26.0296121 C17.2229657,26.3775663 17.4575954,26.7821827 18.116793,26.6552912 C23.2786458,24.9322794 27,20.0633148 27,14.3212723 C27,7.14112562 21.1789496,1.32087331 13.9988029,1.32087331"></path>
            </svg>
          </a>
        </li>
        <li>
          <a class="icon brands social-icon" href="mailto:sosk__AT__preferred__DOT__jp" target="_blank" rel="noopener" title="Email">
            <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
              <path d="M25.2794292,5.59128519 L14,16.8707144 L2.72057081,5.59128519 C3.06733103,5.30237414 3.51336915,5.12857603 4,5.12857603 L24,5.12857603 C24.4866308,5.12857603 24.932669,5.30237414 25.2794292,5.59128519 Z M25.9956978,6.99633695 C25.998551,7.04004843 26,7.08414302 26,7.12857603 L26,20.871424 C26,21.0798433 25.9681197,21.2808166 25.9089697,21.4697335 L18.7156355,14.2763993 L25.9956978,6.99633695 Z M24.9498374,22.6319215 C24.6672737,22.7846939 24.3437653,22.871424 24,22.871424 L4,22.871424 C3.5268522,22.871424 3.09207889,22.7071233 2.74962118,22.432463 L10.0950247,15.0870594 L13.9848068,18.9768415 L14.1878486,18.7737996 L14.2030419,18.7889929 L17.6549753,15.3370594 L24.9498374,22.6319215 Z M2.00810114,21.0526627 C2.00273908,20.9929669 2,20.9325153 2,20.871424 L2,7.12857603 C2,7.08414302 2.00144896,7.04004843 2.00430222,6.99633695 L9.03436454,14.0263993 L2.00810114,21.0526627 Z"></path>
            </svg>
          </a>
        </li>
        <li>
    <a class="icon brands social-icon" href="https://twitter.com/sosk_sosk" target="_blank" rel="noopener" title="Twitter">
        <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
            <path d="M8.991284,24.971612 C19.180436,24.971612 24.752372,16.530224 24.752372,9.210524 C24.752372,8.970656 24.747512,8.731868 24.736496,8.494376 C25.818008,7.712564 26.758256,6.737 27.5,5.62622 C26.507372,6.067076 25.439252,6.364292 24.318752,6.498212 C25.462472,5.812628 26.340512,4.727444 26.754584,3.434036 C25.684088,4.068536 24.499004,4.53002 23.23724,4.778528 C22.226468,3.701876 20.786828,3.028388 19.193828,3.028388 C16.134404,3.028388 13.653536,5.509256 13.653536,8.567492 C13.653536,9.0023 13.702244,9.424904 13.797176,9.830552 C9.19346,9.599108 5.11106,7.39472 2.3792,4.04294 C1.903028,4.861364 1.629032,5.812628 1.629032,6.827072 C1.629032,8.74904 2.606972,10.445612 4.094024,11.438132 C3.185528,11.41016 2.331788,11.160464 1.585184,10.745096 C1.583888,10.768208 1.583888,10.791428 1.583888,10.815728 C1.583888,13.49888 3.493652,15.738584 6.028088,16.246508 C5.562932,16.373084 5.07326,16.44134 4.56782,16.44134 C4.210988,16.44134 3.863876,16.406024 3.526484,16.34144 C4.231724,18.542264 6.276596,20.143796 8.701412,20.18894 C6.805148,21.674696 4.416836,22.56008 1.821488,22.56008 C1.374476,22.56008 0.93362,22.534592 0.5,22.4834 C2.951708,24.054476 5.862524,24.971612 8.991284,24.971612"></path>
        </svg>
    </a>
        </li>
        <li>
          <a class="icon brands social-icon" href="https://www.linkedin.com/in/sosk/" target="_blank" rel="noopener" title="LinkedIn">
            <svg width="28px" height="28px" viewBox="0 0 28 28" version="1.1" fill="#ABABAB" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink">
              <path d="M2,3.654102 C2,2.69908141 2.79442509,1.92397846 3.77383592,1.92397846 L24.2261641,1.92397846 C25.2058917,1.92397846 26,2.69908141 26,3.654102 L26,24.3462148 C26,25.3015521 25.2058917,26.0760215 24.2261641,26.0760215 L3.77383592,26.0760215 C2.79442509,26.0760215 2,25.3015521 2,24.3465315 L2,3.65378524 L2,3.654102 Z M9.27526132,22.1415901 L9.27526132,11.2356668 L5.65030092,11.2356668 L5.65030092,22.1415901 L9.27557808,22.1415901 L9.27526132,22.1415901 Z M7.46341463,9.74691162 C8.72727273,9.74691162 9.51409566,8.90940767 9.51409566,7.86284447 C9.49033893,6.79252455 8.72727273,5.97846056 7.48748812,5.97846056 C6.24675325,5.97846056 5.43649034,6.79252455 5.43649034,7.86284447 C5.43649034,8.90940767 6.22299652,9.74691162 7.4396579,9.74691162 L7.46309788,9.74691162 L7.46341463,9.74691162 Z M11.2815965,22.1415901 L14.9062401,22.1415901 L14.9062401,16.0519481 C14.9062401,15.7263225 14.9299968,15.4000634 15.0256573,15.1675641 C15.2876148,14.5159962 15.8840672,13.8416218 16.8856509,13.8416218 C18.1970225,13.8416218 18.7218879,14.8416218 18.7218879,16.3078872 L18.7218879,22.1415901 L22.3465315,22.1415901 L22.3465315,15.8885017 C22.3465315,12.5388027 20.5584416,10.9800443 18.1735825,10.9800443 C16.2182452,10.9800443 15.3595185,12.072854 14.8824834,12.8172315 L14.9065569,12.8172315 L14.9065569,11.2359835 L11.2819132,11.2359835 C11.3291099,12.2591067 11.2815965,22.1419069 11.2815965,22.1419069 L11.2815965,22.1415901 Z"></path>
            </svg>
          </a>
        </li>
          </ul>
    </div>
    
  </header><!-- Main -->
  <div id="main">
    <!-- One -->
    <section id="one">
      <!-- <header class="major">
        <h2></h2>
      </header> -->
      <h2 style="display:inline">Sosuke Kobayashi</h2><!--
        <a href="https://scholar.google.com/citations?user=VY6PqvsAAAAJ" class="icon brands fa-google"><span class="label">Google Scholar</span></a>
        <a href="https://github.com/soskek" class="icon brands fa-github"><span class="label">Github</span></a>
        <a href="mailto:sosk_atatat_preferred_dotdotdot_jp" class="icon solid fa-envelope"><span class="label">sosk atatat preferred dotdotdot jp</span></a>
        <a href="https://twitter.com/sosk_sosk" class="icon brands fa-twitter"><span class="label">Twitter</span></a>-->
        <br>

        I'm a researcher on machine learning, NLP, and 3D/2D computer vision at <a href=
      "https://www.preferred-networks.jp/en/">Preferred Networks,
      Inc.</a> and a part-time researcher at <a href= "https://www.fai.cds.tohoku.ac.jp/">FaiLab</a> in Tohoku University.
        I've explored next-generation machine learning technologies in various fields.
        Publications through my PhD and R&D are involved with
        <ul>
	  <li>neural scene representations with versatile features (<a href="https://pfnet-research.github.io/distilled-feature-fields/">NeurIPS 2022</a>) and real-time volumetric videos (<a href="https://www.youtube.com/watch?v=xbngQWtmtQ8">Metaverse EXPO 2022</a>)</li>
          <li>analyzing influence of training data efficiently (<a href="https://arxiv.org/abs/2012.04207">SustaiNLP 2020</a>, JNLP 2021)</li>
	  <li>subnetworks for building diverse models at once (<a href="https://openreview.net/forum?id=rCzgE3zHL-q">BigScience 2022</a>, <a href="https://arxiv.org/abs/2012.04207">SustaiNLP 2020</a>, JNLP 2021)</li>
          <li>neural networks with random parameters (<a href="https://arxiv.org/abs/2012.04207">SustaiNLP 2020</a>, JNLP 2021, <a href="https://arxiv.org/abs/2004.12073">Takase and me. NeurIPS 2020</a>)</li>
          <li>controlling robots/agents with language or other interactions (<a href="https://arxiv.org/abs/1710.06280">ICRA 2018 [Best Paper in HRI]</a>, <a href="https://vigilworkshop.github.io/static/papers/3.pdf">Fu et al. ViGIL 2019</a>, <a href="https://arxiv.org/abs/1810.11748">Arakawa et al. RT-DUNE 2019</a>, <a href="https://blogs.nvidia.com/blog/2019/03/20/home-helper-startups-robot-can-tidy-up-a-messy-house/">GTC 2019</a>)</li>
          <li>model-based data augmentation (<a href="http://aclweb.org/anthology/N18-2072.pdf">NAACL 2018</a>)</li>
          <li>entity-centric representations (<a href="http://aclweb.org/anthology/N16-1099.pdf">NAACL 2016</a>, <a href="http://aclweb.org/anthology/I17-1048.pdf">IJCNLP 2017</a>)</li>
        </ul>
        Other collaborations with my friends and collegues are mainly as follows
        <ul>
	  <li>improving Transformer architecture and training (<a href="https://arxiv.org/abs/2206.00330">Takase et al. arXiv 2022</a>)</li>
	  <li>machine translation for language learning (<a href="https://doi.org/10.1145/3491102.3501839">Arakawa et al. CHI 2022</a>)</li>
          <li>shift invariance and positional encoding in Transformers (<a href="https://arxiv.org/abs/2109.05644">Kiyono et al. EMNLP 2021</a>)</li>
          <li>instance-based learning (<a href="https://arxiv.org/abs/2004.14514">Ouchi et al. ACL 2020</a>, <a href="https://arxiv.org/abs/2109.13497">Ouchi et al. TACL 2021</a>)</li>
          <li>generative adversarial networks for video generation (<a href="https://arxiv.org/abs/1811.09245">Saito et al. IJCV 2020</a>)</li>
          <li>connecting the dots of test-time inference and data augmentation (<a href="https://arxiv.org/abs/1906.08412">Shimada et al. LLD 2019</a>)</li>
          <li>co-occurrence estimation (<a href="https://arxiv.org/abs/1809.00800">Yokoi et al. EMNLP 2018</a>)</li>
          <li>controlling text style
            (<a href="http://aclweb.org/anthology/I17-2069.pdf">Akama et al. IJCNLP 2017</a>,
            <a href="http://aclweb.org/anthology/P18-2091.pdf">Akama et al. ACL 2018</a>)</li>
        </ul>
        <!-- <ul class="actions">
        <li><a href="#" class="button">Learn More</a></li>
      </ul> -->
      
    </section><!-- Two -->
    <section id="two">
      <h2>Refereed Publications and Preprints</h2>
      (or see <a href=
          "https://scholar.google.com/citations?user=VY6PqvsAAAAJ"
          class="icon brands fa-google"><span class=
          "label">Google Scholar</span>oogle Scholar</a>)<br><br>
      <div class="row">
	<article class="col-12 col-12-xsmall work-item">
          <h3>B2T Connection: Serving Stability and Performance in Deep Transformers</h3>
          <p>Sho Takase, Shun Kiyono, <u>Sosuke Kobayashi</u>, Jun Suzuki</p>
          <p>In Proceedings of Findings of ACL 2023, July 2023. [<a href="https://arxiv.org/abs/2206.00330">arxiv</a>]</p>
        </article>
	<article class="col-12 col-12-xsmall work-item">
          <h3>Decomposing NeRF for Editing via Feature Field Distillation</h3>
          <p><u>Sosuke Kobayashi</u>, Eiichi Matsumoto, Vincent Sitzmann</p>
          <p>In Proceedings of NeurIPS 2022, Nov. 2022. [<a href="https://arxiv.org/abs/2205.15585">arxiv</a>] [<a href="https://github.com/pfnet-research/distilled-feature-fields">code</a>] [<a href="https://pfnet-research.github.io/distilled-feature-fields/">project</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Diverse Lottery Tickets Boost Ensemble from a Single Pretrained Model</h3>
          <p><u>Sosuke Kobayashi</u>, Shun Kiyono, Jun Suzuki, Kentaro Inui</p>
          <p>In Proceedings of Workshop on Challenges & Perspectives in Creating Large Language Models (BigScience), May 2022. [<a href="https://aclanthology.org/2022.bigscience-1.4/">paper</a>] [<a href="https://arxiv.org/abs/2205.11833">arxiv</a>] [<a href="https://aclanthology.org/2022.bigscience-1.4.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>VocabEncounter: NMT-powered Vocabulary Learning by Presenting Computer-Generated Usages of Foreign Words into Users' Daily Lives</h3><!--'-->
          <p>Riku Arakawa, Hiromu Yakura, <u>Sosuke Kobayashi</u></p>
          <p>In Proceedings of CHI 2022, Apr. 2022. [<a href="https://doi.org/10.1145/3491102.3501839">paper</a>] [<a href="./publications/bibs/chi22.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Instance-Based Neural Dependency Parsing</h3>
          <p>Hiroki Ouchi, Jun Suzuki, <u>Sosuke Kobayashi</u>, Sho Yokoi, Tatsuki Kuribayashi, Masashi Yoshikawa, Kentaro Inui</p>
          <p>TACL (Transactions of the Association for Computational Linguistics), Vol. 9, Dec. 2021. [<a href="https://doi.org/10.1162/tacl_a_00439">paper</a>] [<a href="https://arxiv.org/abs/2109.13497">arxiv</a>] [<a href="https://direct.mit.edu/Citation/Download?resourceId=108864&resourceType=3&citationFormat=2">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>SHAPE: Shifted Absolute Position Embedding for Transformers</h3>
          <p>Shun Kiyono, <u>Sosuke Kobayashi</u>, Jun Suzuki, Kentaro Inui</p>
          <p>In Proceedings of EMNLP 2021, Nov. 2021. [<a href="https://aclanthology.org/2021.emnlp-main.266/">paper</a>] [<a href="https://arxiv.org/abs/2109.05644">arxiv</a>] [<a href="https://aclanthology.org/2021.emnlp-main.266.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>*Efficient Estimation of Influence of a Training Instance</h3>
          <p><u>Sosuke Kobayashi</u>, Sho Yokoi, Jun Suzuki, Kentaro Inui</p>
          <p>Journal of Natural Language Processing (*written in Japanese, extended version of the paper at SustaiNLP 2020), Vol. 28-2, June 2021. (論文賞; Best Paper Award) [<a href="https://www.jstage.jst.go.jp/article/jnlp/28/2/28_573/_pdf">paper</a>] [<a href="https://arxiv.org/abs/2012.04207">arxiv</a>] [<a href="https://github.com/soskek/turnover_dropout">code</a>] [<a href="https://www.jstage.jst.go.jp/AF06S010ShoshJkuDld?sryCd=jnlp&noVol=28&noIssue=2&kijiCd=28_573&kijiLangKrke=ja&kijiToolIdHkwtsh=AT0073&request_locale=JA">bib</a>]</p> <!-- 訓練事例の影響の軽量な推定 -->
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Efficient Estimation of Influence of a Training Instance</h3>
          <p><u>Sosuke Kobayashi</u>, Sho Yokoi, Jun Suzuki, Kentaro Inui</p>
          <p>In Proceedings of First Workshop on Simple and Efficient Natural Language Processing (SustaiNLP 2020), Nov. 2020. [<a href="https://www.aclweb.org/anthology/2020.sustainlp-1.6/">paper</a>] [<a href="https://arxiv.org/abs/2012.04207">arxiv</a>] [<a href="https://github.com/soskek/turnover_dropout">code</a>] [<a href="https://www.aclweb.org/anthology/2020.sustainlp-1.6.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>All Word Embeddings from One Embedding</h3>
          <p>Sho Takase, <u>Sosuke Kobayashi</u></p>
          <p>In Proceedings of NeurIPS 2020, Dec. 2020. [<a href="https://proceedings.neurips.cc//paper/2020/hash/275d7fb2fd45098ad5c3ece2ed4a2824-Abstract.html">paper</a>] [<a href=
          "https://arxiv.org/abs/2004.12073">arxiv</a>] [<a href=
          "https://github.com/takase/alone_seq2seq">code</a>] [<a href="https://proceedings.neurips.cc/paper/2020/file/275d7fb2fd45098ad5c3ece2ed4a2824-Bibtex.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Instance-based Learning of Span Representations: A
          Case Study through Named Entity Recognition</h3>
          <p>Hiroki Ouchi, Jun Suzuki, <u>Sosuke Kobayashi</u>, Sho
          Yokoi, Tatsuki Kuribayashi, Ryuto Konno, Kentaro Inui</p>
          <p>In Proceedings of ACL 2020, July 2020.
            [<a href="https://www.aclweb.org/anthology/2020.acl-main.575/">paper</a>] [<a href="https://arxiv.org/abs/2004.14514">arxiv</a>] [<a href="https://www.aclweb.org/anthology/2020.acl-main.575.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN</h3>
          <p>Masaki Saito, Shunta Saito, Masanori Koyama, <u>Sosuke Kobayashi</u></p>
          <p>International Journal of Computer Vision, May 2020.
            [<a href="https://doi.org/10.1007/s11263-020-01333-y">paper</a>] [<a href="https://arxiv.org/abs/1811.09245">arxiv</a>] [<a href="https://github.com/pfnet-research/tgan2">code</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Learning from Observation-Only Demonstration for
          Task-Oriented Language Grounding via
          Self-Examination</h3>
          <p>Tsu-Jui Fu, Yuta Tsuboi, <u>Sosuke Kobayashi</u>, Yuta
          Kikuchi</p>
          <p>In Proceedings of 3rd Workshop on Visually Grounded
          Interaction and Language (ViGIL), Dec. 2019. [<a href=
          "https://tsujuifu.github.io/pubs/neuripsw19_ood.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Data Interpolating Prediction: Alternative
          Interpretation of Mixup</h3>
          <p>Takuya Shimada, Shoichiro Yamaguchi, Kohei Hayashi,
          <u>Sosuke Kobayashi</u></p>
          <p>In Proceedings of 2nd Workshop on Learning with
          Limited Labeled Data: Weak Supervision and Beyond (LLD),
          May 2019. [<a href=
          "https://openreview.net/pdf?id=rygEej0Eu4">paper</a>]
          [<a href=
          "https://arxiv.org/abs/1906.08412">arxiv</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>DQN-TAMER: Human-in-the-Loop Reinforcement Learning
          with Intractable Feedback</h3>
          <p>Riku Arakawa, <u>Sosuke Kobayashi</u>, Yuya Unno, Yuta
          Tsuboi, Shin-ichi Maeda</p>
          <p>In Proceedings of 2nd Workshop on Human-Robot Teaming
          Beyond Human Operational Speeds and Robot Teammates
          Operating in Dynamic, Unstructured Environments
          (RT-DUNE), May 2019. [<a href=
          "https://arxiv.org/abs/1810.11748">arxiv</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Pointwise HSIC: A Linear-Time Kernelized
          Co-occurrence Norm for Sparse Linguistic Expressions</h3>
          <p>Sho Yokoi, <u>Sosuke Kobayashi</u>, Kenji Fukumizu,
          Jun Suzuki, Kentaro Inui</p>
          <p>In Proceedings of EMNLP 2018, Oct. 2018. [<a href=
          "https://www.aclweb.org/anthology/D18-1203.pdf">paper</a>]
          [<a href="https://arxiv.org/abs/1809.00800">arxiv</a>]
          [<a href=
          "https://speakerdeck.com/eumesy/pointwise-hsic-a-linear-time-kernelized-co-occurrence-norm-for-sparse-linguistic-expressions">slide</a>]
          [<a href=
          "https://www.aclweb.org/anthology/D18-1203.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Unsupervised Learning of Style-sensitive Word
          Vectors</h3>
          <p>Reina Akama, Kento Watanabe, Sho Yokoi, <u>Sosuke
          Kobayashi</u>, Kentaro Inui</p>
          <p>In Proceedings of ACL 2018, July 2018. [<a href=
          "https://www.aclweb.org/anthology/P18-2091.pdf">paper</a>]
          [<a href="https://arxiv.org/abs/1805.05581">arxiv</a>]
          [<a href=
          "https://github.com/jqk09a/style-sensitive-word-vectors">code</a>]
          [<a href=
          "https://www.aclweb.org/anthology/P18-2091.bib">bib</a>]
          [<a href=
          "https://jqk09a.github.io/style-sensitive-word-vectors/">project</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Contextual Augmentation: Data Augmentation by Words
          with Paradigmatic Relations</h3>
          <p><u>Sosuke Kobayashi</u></p>
          <p>In Proceedings of NAACL HLT 2018, June 2018. [<a href=
          "https://www.aclweb.org/anthology/N18-2072.pdf">paper</a>]
          [<a href="https://arxiv.org/abs/1805.06201">arxiv</a>]
          [<a href=
          "https://github.com/pfnet-research/contextual_augmentation">code</a>]
          [<a href=
          "https://www.aclweb.org/anthology/N18-2072.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Interactively Picking Real-World Objects with
          Unconstrained Spoken Language Instructions</h3>
          <p>Jun Hatori*, Yuta Kikuchi*, <u>Sosuke Kobayashi</u>*,
          Kuniyuki Takahashi*, Yuta Tsuboi*, Yuya Unno*, Wilson Ko,
          Jethro Tan (* equally contributed)</p>
          <p>In Proceedings of International Conference on Robotics
          and Automation (ICRA) 2018, June 2018. (Best Paper Award
          on Human-Robot Interaction (HRI)) [<a href=
          "https://arxiv.org/pdf/1710.06280.pdf">paper</a>]
          [<a href="https://arxiv.org/abs/1710.06280">arxiv</a>]
          [<a href=
          "https://github.com/pfnet-research/picking-instruction">dataset</a>]
          [<a href=
          "https://projects.preferred.jp/interactive-robot/#cite">bib</a>]
          [<a href=
          "https://projects.preferred.jp/interactive-robot/">project</a>]
          [<a href=
              "https://www.youtube.com/watch?v=DGJazkyw0Ws&amp;feature=youtu.be">video</a>]</p>
          <p>A subsequent work of this is on <a href="https://projects.preferred.jp/tidying-up-robot/en/">an autonomous robot system demo</a>
            (<a href="https://www.wsj.com/articles/attention-parents-this-robot-will-clean-up-your-childs-toys-1539602386">WSJ</a>,
            <a href="http://www.bbc.com/future/bespoke/the-disruptors/making-a-better-future/">BBC</a>,
            <a href="https://blogs.nvidia.com/blog/2019/03/20/home-helper-startups-robot-can-tidy-up-a-messy-house/">NVIDIA GTC 2019</a>)</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>A Neural Language Model for Dynamically Representing
          the Meanings of Unknown Words and Entities in a
          Discourse</h3>
          <p><u>Sosuke Kobayashi</u>, Naoaki Okazaki, Kentaro
          Inui</p>
          <p>In Proceedings of IJCNLP 2017, Nov. 2017. [<a href=
          "https://www.aclweb.org/anthology/I17-1048.pdf">paper</a>]
          [<a href="https://arxiv.org/abs/1709.01679">arxiv</a>]
          [<a href=
          "./publications/slide_kobayashi_ijcnlp2017.pdf">slide</a>]
          [<a href=
          "https://www.aclweb.org/anthology/I17-1048.bib">bib</a>]
          [<a href=
          "https://raw.githubusercontent.com/soskek/dynamic_neural_text_model/master/misc/dynamic_entity_representation_in_hikaru_no_go.gif">video</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Generating Stylistically Consistent Dialog Responses
          with Transfer Learning</h3>
          <p>Reina Akama, Kazuaki Inada, Naoya Inoue, <u>Sosuke
          Kobayashi</u>, Kentaro Inui</p>
          <p>In Proceedings of IJCNLP 2017, Nov. 2017. [<a href=
          "https://www.aclweb.org/anthology/I17-2069.pdf">paper</a>]
          [<a href=
          "https://www.aclweb.org/anthology/I17-2069.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>An RNN-based Binary Classifier for the Story Cloze
          Test</h3>
          <p>Melissa Roemmele, <u>Sosuke Kobayashi</u>, Naoya
          Inoue, Andrew Gordon</p>
          <p>In Proceedings of 2nd Workshop on Linking Models of
          Lexical, Sentential and Discourse-level Semantics
          (LSDSem), Apr. 2017. [<a href=
          "https://www.aclweb.org/anthology/W17-0911.pdf">paper</a>]
          [<a href=
          "https://www.aclweb.org/anthology/W17-0911.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Explaining Potential Risks in Traffic Scenes by
          Combining Logical Inference and Physics Simulation</h3>
          <p>Ryo Takahashi, Naoya Inoue, Yasutaka Kuriya, <u>Sosuke
          Kobayashi</u>, Kentaro Inui</p>
          <p>International Journal of Machine
          Learning and Computing (IJMLC), Oct. 2016. [<a href=
          "http://www.ijmlc.org/vol6/606-A7.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Tohoku at SemEval-2016 Task 6: Feature-based Model
          versus Convolutional Neural Network for Stance
          Detection</h3>
          <p>Yuki Igarashi, Hiroya Komatsu, <u>Sosuke
          Kobayashi</u>, Naoaki Okazaki, Kentaro Inui</p>
          <p>In Proceedings of 10th International Workshop on
          Semantic Evaluation (SemEval 2016), June 2016. [<a href=
          "https://www.aclweb.org/anthology/S16-1065.pdf">paper</a>]
          [<a href=
          "https://www.aclweb.org/anthology/S16-1065.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Dynamic Entity Representation with Max-pooling
          Improves Machine Reading</h3>
          <p><u>Sosuke Kobayashi</u>, Ran Tian, Naoaki Okazaki,
          Kentaro Inui</p>
          <p>In Proceedings of NAACL HLT 2016, June 2016. [<a href=
          "https://www.aclweb.org/anthology/N16-1099.pdf">paper</a>]
          [<a href=
          "https://github.com/soskek/der-network">code</a>]
          [<a href=
          "https://www.aclweb.org/anthology/N16-1099.bib">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Recognizing Potential Traffic Risks through
          Logic-based Deep Scene Understanding</h3>
          <p>Naoya Inoue, Yasutaka Kuriya, <u>Sosuke Kobayashi</u>,
          Kentaro Inui</p>
          <p>In Proceedings of 22nd ITS World Congress, Oct. 2015.
          (Best of the Rest) [<a href=
          "http://www.cl.ecei.tohoku.ac.jp/~naoya-i/resources/itswc2015_paper.pdf">paper</a>]</p>
        </article>
      </div>
    </section><!-- Three -->
    <section id="three">
      <h2>Other Publications</h2> (mainly in Japanese)
      <div class="row">
        <!--<article class="col-12 col-12-xsmall work-item" style="margin: 0 0 1em 0;">-->
        <article class="col-12 col-12-xsmall work-item">
          <h3>Transformer を多層にする際の勾配消失問題と解決法について</h3>
          <p>高瀬翔, 清野舜, <u>小林颯介</u>, 鈴木潤</p>
          <p>In Proceedings of 言語処理学会第28回年次大会, Mar. 2022. (優秀賞; Outstanding Paper Award) [<a href="https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/A2-5.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>シフト付き絶対位置埋め込み</h3>
          <p>清野舜, <u>小林颯介</u>, 鈴木潤, 乾健太郎</p>
          <p>In Proceedings of 言語処理学会第28回年次大会, Mar. 2022. (若手奨励賞; Young Researcher Award) [<a href="https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/PH2-7.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>事例ベース依存構造解析のための依存関係表現学習</h3>
          <p>大内啓樹, 鈴木潤, <u>小林颯介</u>, 横井祥, 栗林樹生, 吉川将司, 乾健太郎</p>
          <p>In Proceedings of 言語処理学会第27回年次大会, Mar. 2021. [<a href="https://www.anlp.jp/proceedings/annual_meeting/2021/pdf_dir/D3-4.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>単一事例エキスパートの統合によるドメイン適応</h3>
          <p>清野舜, <u>小林颯介</u>, 鈴木潤, 乾健太郎</p>
          <p>In Proceedings of 言語処理学会第27回年次大会, Mar. 2021. [<a href="https://www.anlp.jp/proceedings/annual_meeting/2021/pdf_dir/B1-4.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>スパン間の類似性に基づく事例ベース構造予測</h3>
          <p>大内啓樹, 鈴木潤, <u>小林颯介</u>, 横井祥, 栗林樹生, 乾健太郎</p>
          <p>In Proceedings of 言語処理学会第26回年次大会, Mar. 2020. [<a href=
          "https://www.anlp.jp/proceedings/annual_meeting/2020/pdf_dir/D1-1.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>実世界での話し言葉指示による物体移動：深層学習による画像・言語理解</h3>
          <p>高橋城志*, 羽鳥潤*, 菊池悠太*, <u>小林颯介</u>*, 坪井祐太*, 海野裕也*, 中島統太郎,
          福田昌昭, Wilson Ko, Jethro Tan (* equally contributed)</p>
          <p>In Proceedings of 第36回日本ロボット学会学術講演会, Sep. 2018.
          (研究奨励賞; Young Investigation Excellence Award) [<a href=
          "./publications/rsj2018-nlp-robot.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>カーネル法に基づく疎な言語表現のための高速計算可能な共起尺度</h3>
          <p>横井祥, <u>小林颯介</u>, 福水健次, 乾健太郎</p>
          <p>In Proceedings of 第32回人工知能学会全国大会, June 2018. (全国大会優秀賞;
          JSAI Annual Conference Award) [<a href=
          "https://confit.atlas.jp/guide/event-img/jsai2018/1J2-01/public/pdf?type=in">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>スタイルの類似性を捉えた単語ベクトルの教師なし学習</h3>
          <p>赤間怜奈, 渡邉研斗, 横井祥, <u>小林颯介</u>, 乾健太郎</p>
          <p>In Proceedings of 第32回人工知能学会全国大会, June 2018. (全国大会優秀賞;
          JSAI Annual Conference Award) [<a href=
          "https://confit.atlas.jp/guide/event-img/jsai2018/1N2-03/public/pdf?type=in">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>実世界におけるインタラクティブな物体指示</h3>
          <p>羽鳥潤*, 菊池悠太*, <u>小林颯介</u>*, 高橋城志*, 坪井祐太*, 海野裕也*, Wilson
          Ko, Jethro Tan (* equally contributed)</p>
          <p>In Proceedings of 言語処理学会第24回年次大会, Mar. 2018. [<a href=
          "https://anlp.jp/proceedings/annual_meeting/2018/pdf_dir/C5-1.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>カーネル法に基づく疎な言語表現のための共起尺度</h3>
          <p>横井祥, <u>小林颯介</u>, 福水健次, 乾健太郎</p>
          <p>In Proceedings of 言語処理学会第24回年次大会, Mar. 2018. [<a href=
          "https://anlp.jp/proceedings/annual_meeting/2018/pdf_dir/C7-1.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>サンプリング戦略に基づく単語ベクトルの意味成分とスタイル成分の分離</h3>
          <p>赤間怜奈, 横井祥, 渡邉研斗, <u>小林颯介</u>, 田然, 乾健太郎</p>
          <p>In Proceedings of 言語処理学会第24回年次大会, Mar. 2018. [<a href=
          "https://anlp.jp/proceedings/annual_meeting/2018/pdf_dir/D4-3.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>カーネル法に基づく共起尺度</h3>
          <p>横井祥, 福水健次, <u>小林颯介</u>, 乾健太郎</p>
          <p>In Proceedings of 第20回情報論的学習理論ワークショップ (IBIS), Nov.
          2017. [<a href=
          "http://www.cl.ecei.tohoku.ac.jp/~yokoi/docs/201711-ibis-poster.pdf">slide</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>対話返答生成における個性の追加反映</h3>
          <p>濱田晃一, 藤川和樹, <u>小林颯介</u>, 菊池悠太, 海野裕也, 土田正明</p>
          <p>In Proceedings of 第232回 情報処理学会 自然言語処理研究会, July 2017.
          [<a href=
          "https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&amp;page_id=13&amp;block_id=8&amp;item_id=182719&amp;item_no=1">paper</a>]
          [<a href=
          "https://www.slideshare.net/hamadakoichi/neural-dialogue-generation">slide</a>]
          [<a href=
          "https://ipsj.ixsq.nii.ac.jp/ej/?action=repository_bibtex&amp;itemId=182719&amp;itemNo=1">bib</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>転移学習を用いた対話応答のスタイル制御</h3>
          <p>赤間怜奈, 稲田和明, <u>小林颯介</u>, 佐藤祥多, 乾健太郎</p>
          <p>In Proceedings of 言語処理学会第23回年次大会, Mar. 2017. [<a href=
          "https://www.anlp.jp/proceedings/annual_meeting/2017/pdf_dir/B3-3.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>Recognizing Potential Traffic Risks through
          Logic-based Deep Scene Understanding</h3>
          <p>Shunya Maruta, Yasutaka Kuriya, Naoya Inoue, <u>Sosuke
          Kobayashi</u>, Kentaro Inui</p>
          <p>In Proceedings of Denso Technical Review, 2016.
          (Modified version of the paper of 22nd ITSWC) [<a href=
          "https://ci.nii.ac.jp/naid/40021053584">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>共参照関係に基づく分散表現の共有と動的更新</h3>
          <p><u>小林颯介</u>, 岡崎直観, 乾健太郎</p>
          <p>In Proceedings of NLP若手の会 (YANS) 第11回シンポジウム, Aug.
          2016. (奨励賞; Encouragement Award)</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>談話内における局所文脈の動的分散表現</h3>
          <p><u>小林颯介</u>, 田然, 岡崎直観, 乾健太郎</p>
          <p>In Proceedings of 言語処理学会第22回年次大会, Mar. 2016. (優秀賞;
          Outstanding Paper Award) [<a href=
          "https://www.anlp.jp/proceedings/annual_meeting/2016/pdf_dir/A7-5.pdf">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>再帰型ニューラルネットワークを用いた対話破綻検出と言語モデルのマルチタスク学習</h3>
          <p><u>小林颯介</u>, 海野裕也, 福田昌昭</p>
          <p>In Proceedings of 第75回 人工知能学会 言語・音声理解と対話処理研究会, Oct.
          2015. [<a href=
          "https://jsai.ixsq.nii.ac.jp/ej/?action=pages_view_main&amp;active_action=repository_view_main_item_detail&amp;item_id=760&amp;item_no=1&amp;page_id=13&amp;block_id=23">paper</a>]</p>
        </article>
        <article class="col-12 col-12-xsmall work-item">
          <h3>物理モデルと論理推論の統合による運転シーンの潜在的危険の予測</h3>
          <p><u>小林颯介</u>, 井之上直也, 栗谷康隆, 近藤敏之, 安部克則, 奥野英一, 乾健太郎</p>
          <p>In Proceedings of 自動車技術会 2015年春季大会学術講演会講演予稿集, May
          2015.</p>
        </article>
      </div>
    </section><!-- Three -->
    <!--
    <section id="three">
      <h2>Others</h2>
      <ul class="alt">
        <li>Dolor pulvinar etiam magna etiam.</li>
        <li>Sagittis adipiscing lorem eleifend.</li>
      </ul>
    </section>--><!-- Three -->
    <section id="last">
      <h2>Other Activities</h2>
      <div class="column col-12 col-xs-12">
        <div class="card">
          <div class="card-body">
            <h5>Education and Experience</h5>
            <ul style="font-size: 80%;">
              <li>2021.11 - <b>current</b>: Researcher (part-time) at <a href=
              "https://www.fai.cds.tohoku.ac.jp/">FaiLab</a> in Tohoku University, Sendai (remote). Research on machine learning and NLP.
              </li>
              <li>2018.10 - 2021.9: Doctor of Information Science at <a href=
              "https://www.nlp.ecei.tohoku.ac.jp/">Inui-Suzuki
              Lab</a> in Graduate School of Information Sciences, Tohoku University, Sendai.
              </li>
              <li>2016.10 - <b>current</b>: Researcher at <a href=
              "https://www.preferred-networks.jp/en/">Preferred
              Networks, Inc.</a>, Tokyo. Research and development on machine learning, 2D/3D computer vision, NLP, robotics, etc.
              </li>
              <li>2014.10 - 2016.9: Master of Information Science at
              <a href=
              "http://www.cl.ecei.tohoku.ac.jp/">Inui-Okazaki
              Lab</a> in 
                Graduate School of Information Sciences, Tohoku
                University, Sendai.<!-- (東北大学大学院情報科学研究科)-->
              </li>
              <li>2011.4 - 2014.9: Department of Information and
              Intelligent Systems, Tohoku University, Sendai.
              (<b>Early graduation</b>)<!-- (東北大学工学部情報知能システム総合学科)--></li>
              <li>2008.4 - 2011.3: Suzaka High School, Nagano.<!--
              (長野県須坂高等学校)--></li>
            </ul>
            <h5>Internship and Part-time Work</h5>
            <ul style="font-size: 80%;">
              <li>
                <b>Part-time Work</b> at <a href=
                "https://preferred.jp/en/">Preferred
                Infrastructure, Inc.</a>, <b>Neural Dialogue
                Response Generation and Multimodal NLP</b>, Oct.
                2015, Feb. and Mar. 2016.
              </li>
              <li>
                <b>Internship</b> at <a href=
                "https://preferred.jp/en/">Preferred
                Infrastructure, Inc.</a>, <b>Neural Dialogue
                Response Generation</b>, Aug. and Sep. 2015.
              </li>
              <li>
                <b>Part-time Work</b> at <a href=
                "http://www.recruitjobs.co.jp/">Recruit Jobs Co.,
                Ltd.</a>, <b>Data Analysis and Development for Web
                Services</b>, 2015.
              </li>
              <li>
                <b>Internship</b> at <a href=
                "http://www.recruit-rgf.com/">Recruit Holdings Co.,
                Ltd.</a>, <b>Data Analysis and Development for Web
                Services</b>, Feb. 2015. <i>(最優秀賞; Best Team
                Award)</i>
              </li>
              <li>
                <b>Collaborative Research</b> with <a href=
                "http://www.denso.co.jp/">DENSO Corporation</a>,
                <b>Recognizing Potential Risks in Traffic
                Scenes</b>, 2014--2015
              </li>
            </ul>
            <h5>Competition</h5>
            <ul style="font-size: 80%;">
              <li>
                20th/946, solo silver medal, at <a href=
                "https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/discussion/288045">chaii - Hindi and Tamil Question Answering</a> in Kaggle, Nov. 2021. (First time participating)
              </li>
            </ul>
            <h5>Review</h5>
            <ul style="font-size: 80%;">
              <li>
                <b>Paper review</b> of NLP Conferences/Journals:
		<a href="https://aclrollingreview.org/">ACL Rolling Review</a> (2021-2022),
		<a href="https://2021.aclweb.org/">ACL</a> (2018-2021),
		<a href="https://2020.emnlp.org/">EMNLP</a> (2018-2020),
		<a href="https://naacl2019.org/">NAACL</a> (2019),
		<a href="https://coling2022.org/">COLING</a> [Area Chair] (2022), <a href="https://www.jstage.jst.go.jp/browse/jnlp/-char/en/">論文誌 自然言語処理</a></font> (2017, 2020-2022)
              </li>
              <li>
		<b>Paper review</b> of ML Conferences:
                <a href="https://nips.cc/Conferences/2022/">NeurIPS</a> (2022),
                <a href="https://icml.cc/Conferences/2022/">ICML</a> (2022)
              </li>
              <li>
                <b>Paper review</b> of Robotics Conference:
		<a href="https://www.icra2019.org/">ICRA</a> (2019)
              </li>
              <li>
                <b>Review and revision</b> of <a href=
                "https://tutorials.chainer.org/">Deep Learning
                Tutorial with Chainer</a>, 2019.
              </li>
              <li>
                <b>Review</b> of <a href=
                "https://www.kspub.co.jp/book/detail/1529243.html">深層学習による自然言語処理
                (機械学習プロフェッショナルシリーズ)</a> <font color="lightgray">(Natural
                Language Processing by Deep Learning, A Series of
                Machine Learning Professionals)</font>, 坪井祐太, 海野裕也,
                鈴木潤, 講談社, 2017. [<a href=
                "https://www.amazon.co.jp/dp/4061529242">amazon</a>]
              </li>
              <li>
                <b>Review</b> of <a href=
                "http://www.kspub.co.jp/book/detail/1529182.html">ウェブデータの機械学習
                (機械学習プロフェッショナルシリーズ)</a> <font color="lightgray">(Web
                Data and Machine Learning, A Series of Machine
                Learning Professionals)</font>, ダヌシカボレガラ, 岡﨑直観,
                前原貴憲, 講談社, 2016. [<a href=
                "https://www.amazon.co.jp/dp/4061529188">amazon</a>]
              </li>
              <li>
                <b>Review</b> of <a href=
                "http://www.kindaikagaku.co.jp/information/kd0487.htm">
                深層学習 Deep Learning</a>, 神嶌敏弘 (編), 麻生英樹, 安田宗樹, 前田新一,
                岡野原大輔, 岡谷貴之, 久保陽太郎, ボレガラダヌシカ, 人工知能学会監修, 近代科学社,
                2015. [<a href=
                "https://www.amazon.co.jp/dp/476490487X">amazon</a>]
              </li>
            </ul>
            <h5>Research/Teaching Assistant</h5>
            <ul style="font-size: 80%;">
              <li>
                <b>Research Assistant</b> of JST, CREST, <a href=
                "https://www.jst.go.jp/kisoken/crest/project/44/15656596.html">
                構造理解に基づく大規模文献情報からの知識発見</a>, Dec. 2015--Sep. 2016.
              </li>
              <li><b>Teaching Assistant (Research Mentor)</b> of
              Team-based Engineering Design Course (for
              underguraduates) of Tohoku University, <b>Vector
              Representations of Emoji (Emoticons) on
              Microblogs</b>, Autumn--Winter 2015--2016.</li>
              <li><b>Teaching Assistant</b> of Information
              Communication Theory and Natural Language Processing
              (for guraduates) of Tohoku University, Autumn--Winter
              2015--2016.</li>
              <li><b>Teaching Assistant</b> of Programming Practice
              A (for undergraduates) of Tohoku University,
              Spring--Summer 2015.</li>
              <li><b>Teaching Assistant (Research Mentor)</b> of
              Team-based Engineering Design Course (for
              underguraduates) of Tohoku University, <b>Knowledge
              Extraction from Web for Question Answering</b>,
              Autumn--Winter 2014--2015.</li>
            </ul>
            <h5>Talk</h5>
            <ul style="font-size: 80%;">
              <li>
                <b>Talk</b> about <a href="https://arxiv.org/abs/2012.04207">Efficient Estimation of Influence of a Training Instance</a> (best paper invited talk of JNLP) at
                the 28th Annual Meeting of the Association for Natural Language Processing (remote), Mar. 2022.
              </li>
              <li>
                <b>Talk</b> about an efficient estimation of machine learning models trained without an instance at
                <a href="https://www.is.tohoku.ac.jp/jp/news/award_s/detail---id-873.html">the 22nd Doctoral Student Presentations</a> of GSIS, Tohoku University, Dec. 2019. (Best Presentation Award)
              </li>
              <li>
                <b>Talk</b> about <a href=
                "https://www.slideshare.net/SSII_Slides/ssii2019os13-149132279">
                ビジョン＆ランゲージによる意図理解と曖昧性解消</a> (Vision and Language
                Help Intent Understanding and its Disambiguation)
                at <a href=
                "https://confit.atlas.jp/guide/event/ssii2019/static/organized#OS1">
                第25回 画像センシングシンポジウム (SSII2019)
                オーガナイズドセッション『ビジョン & ランゲージ
                「意図」をどのようにモデリングするか？』</a>, Jun. 2019
              </li>
              <li>
                <b>Talk</b> about <a href=
                "https://www.slideshare.net/hytae/learning-communication-with-neural-networks">
                Learning Communication with Neural Networks</a> at
                <a href=
                "https://www.youtube.com/watch?v=ZrLiNAMHszo">PFN
                Seminar</a>, Mar. 2017.
              </li>
              <li>
                <b>Talk</b> about <a href=
                "http://naacl.org/naacl-hlt-2016/">NAACL-HLT
                2016</a> at <a href=
                "http://yans.anlp.jp/entry/yans2016">NLP若手の会 (YANS)
                第11回シンポジウム</a> <font color="gray">(11th Symposium
                of Young Researcher Association for NLP
                Studies)</font>, Aug. 2016.
              </li>
              <li>
                <b>Talk</b> about <a href=
                "http://www.slideshare.net/hytae/recent-progress-in-rnn-and-nlp-63762080">
                Recent Progress in RNN and NLP</a> at NLP-DL Study
                Group, Jun. 2016.
              </li>
            </ul>
            <h5>Organizer</h5>
            <ul style="font-size: 80%;">
              <li>
                <b>Organizer</b> of <a href=
                "https://sites.google.com/site/nlpseminarweb">言語処理技術セミナー2021</a> <font color="gray">(NLP Technology Seminar 2021)</font>, Oct. 2021.
              </li>
              <li>
                <b>Organizer</b> of <a href=
                "https://sites.google.com/view/snlp-jp/home/2019">第11回
                最先端NLP勉強会</a> <font color="gray">(11th NLP Paper
                Reading Circle)</font>, Sep. 2019.
              </li>
              <li>
                <b>Organizer</b> of <a href=
                "https://sites.google.com/view/snlp-jp/home/2018">第10回
                最先端NLP勉強会</a> <font color="gray">(10th NLP Paper
                Reading Circle)</font>, Aug. 2018.
              </li>
              <li>
                <b>Organizer</b> of <a href=
                "https://sites.google.com/view/snlp-jp/home/2017">第9回
                最先端NLP勉強会</a> <font color="gray">(9th NLP Paper
                Reading Circle)</font>, Sep. 2017.
              </li>
              <li>
                <b>Organizer</b> of <a href=
                "http://www.logos.t.u-tokyo.ac.jp/snlp8/">第8回
                最先端NLP勉強会</a> <font color="gray">(8th NLP Paper
                Reading Circle)</font>, Sep. 2016.
              </li>
            </ul>
          </div>
        </div>
      </div>
      <div class="column col-12 col-xs-12">
        <div class="card">
          <div class="card-header">
            <div class="card-title h4">
              Github
            </div>
          </div>
          <div class="card-body">
            <ul style="font-size: 80%;">
              <li>Research: Reproduction
                <ul>
                  <li>
                    <a href=
                    "https://github.com/soskek/interval-bound-propagation-chainer">
                    Interval Bound Propagation</a> - <a href=
                    "https://arxiv.org/abs/1810.12715">"Scalable
                    Verified Training for Provably Robust Image
                    Classification" (Goyal et al., 2019)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/learning_to_learn">Meta-learning
                    LSTM Optimizer</a> - <a href=
                    "https://arxiv.org/abs/1606.04474">"Learning to
                    learn by gradient descent by gradient descent"
                    (Andrychowicz et al., 2016)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/bert-chainer">BERT</a>
                    - <a href=
                    "https://arxiv.org/abs/1810.04805">"BERT:
                    Pre-training of Deep Bidirectional Transformers
                    for Language Understanding" (Devlin et al.,
                    2018)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/bookcorpus">Crawler
                    of BookCorpus</a> - <a href=
                    "https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zhu_Aligning_Books_and_ICCV_2015_paper.pdf">
                    "Aligning Books and Movies: Towards Story-like
                    Visual Explanations by Watching Movies and
                    Reading Books" (Zhu et al., 2015)</a> (I'm glad
                    that this code is used in <a href=
                    "https://github.com/NVIDIA/DeepLearningExamples/blob/1187980309bb6dd6fd1dbdd93afb326c6687f9e0/PyTorch/LanguageModeling/BERT/Dockerfile#L24">
                    NVIDIA's BERT with PyTorch</a>!)
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/chainer-openai-transformer-lm">
                    Finetuned Transformer Language Model</a> -
                    <a href=
                    "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">
                    "Improving Language Understanding by Generative
                    Pre-Training" (Radford et al., 2018)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/elmo-chainer">ELMo</a>
                    - <a href=
                    "https://arxiv.org/abs/1802.05365">"Deep
                    contextualized word representations" (Peters et
                    al., 2018)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/dynamic_routing_between_capsules">
                    CapsNet</a> - <a href=
                    "https://arxiv.org/abs/1710.09829" id=
                    "sublink">"Dynamic Routing Between Capsules"
                    (Sabour et al., 2017)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/attention_is_all_you_need">
                    Transformer</a> - <a href=
                    "https://arxiv.org/abs/1706.03762" id=
                    "sublink">"Attention is All You Need" (Vaswani
                    et al., 2017)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/convolutional_seq2seq">
                    Convolutional seq2seq Model</a> - <a href=
                    "https://arxiv.org/abs/1705.03122" id=
                    "sublink">"Convolutional Sequence to Sequence
                    Learning" (Gehring et al., 2017)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/captioning_chainer">Neural
                    Image Caption</a> - <a href=
                    "https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf"
                    id="sublink">"Show and Tell: A Neural Image
                    Caption Generator" (Vinyals et al., 2015)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/skip_thought">Skip-Thought
                    Vectors and Sentence-level RNN Language
                    Model</a> - <a href=
                    "https://arxiv.org/pdf/1506.06726.pdf" id=
                    "sublink">"Skip-Thought Vectors" (Kiros et al.,
                    2015)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/rnnlm_chainer">Document-level
                    RNN Language Model</a> - <a href=
                    "https://arxiv.org/abs/1409.2329" id=
                    "sublink">"Recurrent Neural Network
                    Regularization" (Zaremba et al., 2014)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/sru_language_model">SRU
                    (Simple Recurrent Unit) Language Model and
                    Proposed Variants</a> - <a href=
                    "https://arxiv.org/pdf/1709.02755.pdf" id=
                    "sublink">"Training RNNs as Fast as CNNs." (Lei
                    and Zhang, 2017)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/efficient_softmax">Efficient
                    Softmax Approximation: Adaptive Softmax and
                    BlackOut</a> - <a href=
                    "http://proceedings.mlr.press/v70/grave17a/grave17a.pdf"
                    id="sublink">"Efficient softmax approximation
                    for GPUs" (Grave et al., 2017)</a> and <a href=
                    "https://arxiv.org/abs/1511.06909" id=
                    "sublink">"BlackOut: Speeding up Recurrent
                    Neural Network Language Models With Very Large
                    Vocabularies" (Ji et al., 2016)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/weight_normalization">
                    Weight Normalization</a> - <a href=
                    "https://arxiv.org/abs/1602.07868" id=
                    "sublink">"Weight Normalization: A Simple
                    Reparameterization to Accelerate Training of
                    Deep Neural Networks" (Salimans and Kingma,
                    2016)</a>
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/variational_dropout_sparsifies_dnn">
                    Variational Dropout</a> - <a href=
                    "https://arxiv.org/abs/1701.05369" id=
                    "sublink">"Variational Dropout Sparsifies Deep
                    Neural Networks" (Molchanov et al., 2017)</a>
                  </li>
                </ul>
              </li>
              <li>Research: Original
                <ul>
                  <li>
                    <a href=
                    "https://github.com/soskek/turnover_dropout">
                    Turn-over Dropout</a> - (My work at SustaiNLP
                    2020)
                  </li>
                  <li>
                    <a href=
                    "https://github.com/pfnet-research/contextual_augmentation">
                    Contextual Augmentation</a> - (My work at NAACL
                    2018)
                  </li>
                  <li>
                    <a href=
                    "https://github.com/pfnet-research/picking-instruction">
                    PFN Picking Instructions for Commodities
                    Dataset (PFN-PIC)</a> - (Our work at ICRA 2018)
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/emergence_of_language_using_discrete_sequences">
                    Emergence of Language Using Discrete Sequences
                    with Autoencoder</a> - (My work not published,
                    in 2017)
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/ROCStory_skipthought_baseline">
                    Next Event Predictor</a> - (Related to our work
                    at LSDSem 2017)
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/SDCGAN">Sentence
                    Generater Using Deep Convolutional Generative
                    Adversarial Network (DCGAN)</a> - (My work not
                    published, in 2016)
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/der-network">Dynamic
                    Entity Representation Network</a> - (Our work
                    at NAACL 2016)
                  </li>
                  <li>
                    <a href=
                    "https://github.com/soskek/dynamic_neural_text_model">
                    Dynamic Neural Text Model</a> - (Our work at
                    IJCNLP 2017)
                  </li>
                </ul>
              </li>
              <li>Others
                <ul>
                  <li>
                    <a href=
                    "https://github.com/soskek/arxiv_leaks">ArxivLeaks</a>
                    - Read secret comment of TeX in arXiv papers
                  </li>
                  <li>
                    <a href=
                    "https://github.com/chainer/chainer/search?q=soskek&amp;type=Commits&amp;utf8=%E2%9C%93">
                    Contributions for Chainer, a python-based Deep
                    Learning framework</a>. e.g., Layer
                    normalization, Tree LSTM, seq2seq example and
                    text classification example.
                  </li>
                </ul>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </section>
  </div><!-- Footer -->
  <footer id="footer">

  </footer><!-- Scripts -->
  <script src="assets/js/jquery.min.js"></script> 
  <script src="assets/js/jquery.poptrox.min.js"></script> 
  <script src="assets/js/browser.min.js"></script> 
  <script src="assets/js/breakpoints.min.js"></script> 
  <script src="assets/js/util.js"></script> 
  <script src="assets/js/main.js"></script>
</body>
</html>
